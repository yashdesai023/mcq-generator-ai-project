{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# MCQ Generator with LangChain and Gemini\n",
    "\n",
    "This notebook demonstrates how to build a Multiple Choice Question (MCQ) generator using LangChain and Google's Gemini API. The goal of this project is to automatically generate a quiz from a given text, and then evaluate the generated quiz for quality and relevance.\n",
    "\n",
    "This project is a great way to showcase skills in:\n",
    "\n",
    "*   **Natural Language Processing (NLP):** Using large language models (LLMs) to understand and process text.\n",
    "*   **LangChain:** Building complex applications with LLMs by chaining together different components.\n",
    "*   **API Integration:** Interacting with the Google Generative AI API to leverage powerful models like Gemini.\n",
    "*   **Prompt Engineering:** Designing effective prompts to guide the LLM's output.\n",
    "*   **Python and Jupyter Notebooks:** Writing clean, well-documented code to solve a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f6g7h8i",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75d3ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "Next, we'll load the Google API key from a `.env` file. This is a good practice for managing sensitive information like API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98459676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e7db780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Google API Key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "## 3. LangChain Setup\n",
    "\n",
    "Now we'll set up the core components of our LangChain application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173fa6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "### 3.1. Define Prompt Templates\n",
    "\n",
    "We need two prompt templates: one for generating the MCQs and another for evaluating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c37b6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "Text: {text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7127748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = '''\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "### 3.2. Create LangChain Chains\n",
    "\n",
    "We'll create two `LLMChain` instances: one for quiz generation and one for evaluation. Then, we'll combine them into a `SequentialChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56a0ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13616b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75a2df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "480c9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02dbfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "    chains=[quiz_chain, review_chain],\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    output_variables=[\"quiz\", \"review\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Now we'll load the text from the `data.txt` file. This text will be used as the source material for generating the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12bcfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8bd8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 5. MCQ Generation and Evaluation\n",
    "\n",
    "Now it's time to run our chain and generate the MCQs. We'll specify the number of questions, the subject, and the tone of the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87f72cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 5\n",
    "SUBJECT = \"biology\"\n",
    "TONE = \"simple\"\n",
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\"\n",
    "        },\n",
    "        \"correct\": \"correct answer\"\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\"\n",
    "        },\n",
    "        \"correct\": \"correct answer\"\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\"\n",
    "        },\n",
    "        \"correct\": \"correct answer\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b12c0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain to generate and evaluate the quiz\n",
    "response = generate_evaluate_chain.invoke(\n",
    "    {\n",
    "        \"text\": TEXT,\n",
    "        \"number\": NUMBER,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"tone\": TONE,\n",
    "        \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "### 5.1. Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2354f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- RAW MODEL OUTPUT FOR 'quiz' ---\")\n",
    "print(repr(response['quiz']))\n",
    "print(\"--- END OF RAW OUTPUT ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "## 6. Output Processing\n",
    "\n",
    "Finally, we'll process the output from the LangChain pipeline. We'll parse the generated quiz, format it as a Pandas DataFrame, and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Get the raw string output from the model ---\n",
    "raw_quiz_output = response.get(\"quiz\")\n",
    "print(\"--- RAW MODEL OUTPUT ---\")\n",
    "print(raw_quiz_output)\n",
    "\n",
    "\n",
    "# --- 2. Clean the string to extract only the JSON part ---\n",
    "# We find the first '{' and the last '}' to isolate the JSON object\n",
    "try:\n",
    "    start_index = raw_quiz_output.find('{')\n",
    "    end_index = raw_quiz_output.rfind('}') + 1\n",
    "    cleaned_json_string = raw_quiz_output[start_index:end_index]\n",
    "    \n",
    "    print(\"\\n--- CLEANED JSON STRING ---\")\n",
    "    print(cleaned_json_string)\n",
    "    \n",
    "    # --- 3. Parse the cleaned string ---\n",
    "    quiz_data = json.loads(cleaned_json_string)\n",
    "    print(\"\\n--- SUCCESSFULLY PARSED! ---\")\n",
    "\n",
    "    # --- 4. Continue with the rest of your logic ---\n",
    "    quiz_table_data = []\n",
    "    for key, value in quiz_data.items():\n",
    "        mcq = value[\"mcq\"]\n",
    "        options = \" | \".join(\n",
    "            [\n",
    "                f\"{option}: {option_value}\"\n",
    "                for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "        correct = value[\"correct\"]\n",
    "        quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})\n",
    "\n",
    "    quiz_df = pd.DataFrame(quiz_table_data)\n",
    "    \n",
    "    # Display the final DataFrame\n",
    "    print(\"\\n--- FINAL QUIZ DATAFRAME ---\")\n",
    "    display(quiz_df)\n",
    "\n",
    "    # Save to CSV\n",
    "    quiz_df.to_csv(\"biology_quiz.csv\", index=False)\n",
    "    print(\"\\nSuccessfully saved to biology_quiz.csv\")\n",
    "\n",
    "\n",
    "except (AttributeError, IndexError, json.JSONDecodeError) as e:\n",
    "    print(\"\\n--- FAILED TO CLEAN OR PARSE THE STRING ---\")\n",
    "    print(\"There was an error processing the model's output. It might be in an unexpected format.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
